{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMstMAQwCTzkk+TTudd6RAP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6McRNP0Us_P","executionInfo":{"status":"ok","timestamp":1690193654347,"user_tz":-420,"elapsed":11624,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}},"outputId":"5f3d1f1b-f33f-49db-9928-d8eaa195c44b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Sastrawi\n","  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/209.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Sastrawi\n","Successfully installed Sastrawi-1.0.1\n"]}],"source":["!pip install Sastrawi"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import plotly.express as px\n","import nltk\n","import pickle\n","\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","from google.colab import data_table\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.multioutput import MultiOutputClassifier\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score, KFold\n","\n","data_table.disable_dataframe_formatter()\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0KzN0BoU2s1","executionInfo":{"status":"ok","timestamp":1690193659091,"user_tz":-420,"elapsed":4751,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}},"outputId":"160179c6-456f-4b15-c6bb-c729f9ba0631"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bw6L3VlcU7E2","executionInfo":{"status":"ok","timestamp":1690193690247,"user_tz":-420,"elapsed":31163,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}},"outputId":"39fa255d-860c-49ed-b9b9-ad23d7991138"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["DataPath = \"/content/drive/MyDrive/Magang/ThreatDetection/Data/Model/\""],"metadata":{"id":"0ka5vpa_Wdj_","executionInfo":{"status":"ok","timestamp":1690193690249,"user_tz":-420,"elapsed":13,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["with open(f'{DataPath}AbusiveModel.pkl', 'rb') as file:\n","    AbusiveModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_IndividualModel.pkl', 'rb') as file:\n","    HS_IndividualModel = pickle.load(file)\n","\n","\n","with open(f'{DataPath}HS_GroupModel.pkl', 'rb') as file:\n","    HS_GroupModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_ReligionModel.pkl', 'rb') as file:\n","    HS_ReligionModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_RaceModel.pkl', 'rb') as file:\n","    HS_RaceModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_PhysicalModel.pkl', 'rb') as file:\n","    HS_PhysicalModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_GenderModel.pkl', 'rb') as file:\n","    HS_GenderModel = pickle.load(file)\n","\n","with open(f'{DataPath}HS_OtherModel.pkl', 'rb') as file:\n","    HS_OtherModel = pickle.load(file)\n","\n","with open(f'{DataPath}sexual_orientationModel.pkl', 'rb') as file:\n","    sexual_orientationModel = pickle.load(file)\n","\n","with open(f'{DataPath}disabilityModel.pkl', 'rb') as file:\n","    disabilityModel = pickle.load(file)\n","\n","with open(f'{DataPath}national_originModel.pkl', 'rb') as file:\n","    national_originModel = pickle.load(file)\n","\n","with open(f'{DataPath}violenceModel.pkl', 'rb') as file:\n","    violenceModel = pickle.load(file)"],"metadata":{"id":"5QY5G3aEVPNt","executionInfo":{"status":"ok","timestamp":1690193704393,"user_tz":-420,"elapsed":14156,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["with open(f'{DataPath}fitted_vectorizer.pkl', 'rb') as file:\n","    fitted_vectorizer = pickle.load(file)"],"metadata":{"id":"mevks07hXrCh","executionInfo":{"status":"ok","timestamp":1690193718652,"user_tz":-420,"elapsed":871,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def preprocess(sentence:list):\n","  # case Folding\n","  sentences = [sentence.lower() for sentence in sentence]\n","  # print(sentences)\n","\n","\n","  # tokenizer\n","  tokenizer     = RegexpTokenizer(r'(?<!\\S)[A-Za-z]+(?!\\S)|(?<!\\S)[A-Za-z]+(?=:(?!\\S))')\n","  # hasil_token   = tokenizer.tokenize(sentences[1])\n","  token_sentence = [tokenizer.tokenize(sentence) for sentence in sentences]\n","  # print(token_sentence)\n","\n","\n","  # ah, al, ala, an, and, as, atau, awal, be, ber, bin, bro, bu, bung, but, by, cak, cc, dan, deh, dengan, di, dil, dong, dua, eh, fadli, fahri\n","  # fan, for, front, full, harto, hasan, he, hehe, hem, ia, ih, in, is, itu, iya, je, juga, kah, kak, kan, kang, kat, \"kau\", ke, km, kok, ku, ky\n","  # lah, like, lima, live, \"lo\", me, mei, men, min, mk, mu, my, nah, nak, namun, nek, nih, no, nya, of, oh, oi, oke, -pada, pak, per, ppp, pun, ra\n","  # rang, rt, sa, se, seba, si, sing, ta, tang, tar, tau, the, ting, to, toh, tomohon, uniform, up, untuk, user, wah, wkwk, ya, yaitu, yakni, yang, zon\n","\n","  # yuk,yg,yaa,wkwkwk,tdk,so,kyk,ko,jg,jgn,jd,jdi,hrs,haha,gw,gk,ga,ente,emg,elo,dr,dlm,dgn,byk,bkn,aq,ama,aku,adlh,ad\n","\n","  # ad,adlh,aku,ama,aq,bgt,bkn,blg,bnyk,bs,byk,da,dah,dapet,dapat,dengar,dg,dgn,dll,ga,gmn,gt,gw,haha,hahaha,jd,jdi,jgn,ka,karna,ken,kl,klo,kyk,lg,lho,lu,mmg,mo,msh,ngga,nggak,ntar,yg,yo\n","  words = \"\"\"ad,adlh,aku,ama,aq,bgt,bkn,blg,bnyk,bs,byk,da,dah,dapet,dapat,dengar,dg,dgn,dll,ga,gmn,gt,gw,haha,hahaha,jd,jdi,jgn,ka,karna,ken,kl,klo,kyk,lg,lho,lu,mmg,mo,msh,ngga,nggak,ntar,yg,yo,yuk,yg,yaa,wkwkwk,tdk,so,kyk,ko,jg,jgn,jd,jdi,hrs,haha,gw,gk,ga,ente,emg,elo,dr,dlm,dgn,byk,bkn,aq,ama,aku,adlh,ad,gue,aja,aku,pengguna,ah,al,ala,an,and,as,atau,awal,be,ber,bin,bro,bu,bung,but,by,cak,cc,dan,deh,dengan,di,dil,dong,dua,eh,fadli,fahri,fan,for,front,full,harto,hasan,he,hehe,hem,ia,ih,in,is,itu,iya,je,juga,kah,kak,kan,kang,kat,kau,ke,km,kok,ku,ky,lah,like,lima,live,lo,me,mei,men,min,mk,mu,my,nah,nak,namun,nek,nih,no,nya,of,oh,oi,oke,pak,per,ppp,pun,ra,rang,rt,sa,se,seba,si,sing,ta,tang,tar,ter,tau,the,ting,to,toh,tomohon,uniform,up,untuk,user,wah,wkwk,ya,yaitu,yakni,yang,zon\"\"\"\n","  word = words.split(\",\")\n","  # word = np.reshape(word, -1)\n","  # word.tolist()\n","  # stopword\n","  wordlist= set(stopwords.words('indonesian') + word)\n","  sw_sentences = [[word for word in sentence if word not in wordlist]for sentence in token_sentence]\n","  cleaned_sentences = [' '.join(sentence) for sentence in sw_sentences]\n","  # print(cleaned_sentences)\n","\n","\n","  #[str(sentence) for sentence in sw_sentences]\n","  # stemming\n","  stemmer     = StemmerFactory().create_stemmer()\n","  # input_stemm   = str(sw_sentences)\n","  # hasil_stemm   = stemmer.stem(input_stemm)\n","  # print (hasil_stemm)\n","  stem_sentence = [stemmer.stem(sentence) for sentence in cleaned_sentences]\n","  # print(stem_sentence)\n","  return stem_sentence"],"metadata":{"id":"snKKSQLxV0pc","executionInfo":{"status":"ok","timestamp":1690193719041,"user_tz":-420,"elapsed":7,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["target = \"Abusive\tHS_Individual\tHS_Group\tHS_Religion\tHS_Race\tHS_Physical\tHS_Gender\tHS_Other\tsexual_orientation\tdisability\tnational_origin\tviolence\""],"metadata":{"id":"yvJUlWcLWxIf","executionInfo":{"status":"ok","timestamp":1690193724120,"user_tz":-420,"elapsed":361,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["target = target.split(\"\\t\")"],"metadata":{"id":"ZOJl5EhnYgjU","executionInfo":{"status":"ok","timestamp":1690193725609,"user_tz":-420,"elapsed":9,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = [AbusiveModel, HS_IndividualModel, HS_GroupModel, HS_ReligionModel, HS_RaceModel, HS_PhysicalModel, HS_GenderModel, HS_OtherModel, sexual_orientationModel, disabilityModel, national_originModel, violenceModel]"],"metadata":{"id":"6TsLVdLsbY5T","executionInfo":{"status":"ok","timestamp":1690193726148,"user_tz":-420,"elapsed":2,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KeDHpwjKe181","executionInfo":{"status":"ok","timestamp":1690193727473,"user_tz":-420,"elapsed":9,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}},"outputId":"e1282469-e1bc-49f9-c413-a4add6dc9b0b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[SVC(),\n"," SVC(),\n"," RandomForestClassifier(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier(),\n"," LogisticRegression(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier(),\n"," DecisionTreeClassifier()]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def __main__(kalimat:str):\n","  preprocessed_kalimat = preprocess([kalimat])\n","  kalimat_vector = fitted_vectorizer.transform(preprocessed_kalimat)\n","\n","  classification = []\n","\n","  for idx in range(len(target)):\n","    pred = model[idx].predict(kalimat_vector.toarray())\n","    if pred == 1:\n","      classification.append(target[idx])\n","\n","\n","\n","  json = {'katagori' : classification}\n","  # print(json)\n","  return json\n"],"metadata":{"id":"KJnUYF-gYwYP","executionInfo":{"status":"ok","timestamp":1690193754212,"user_tz":-420,"elapsed":3,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["__main__(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWr1GJnHddsv","executionInfo":{"status":"ok","timestamp":1690194251585,"user_tz":-420,"elapsed":911,"user":{"displayName":"Kurnia Zulda","userId":"17022275670300639963"}},"outputId":"09930d07-aa63-4e8f-f857-904523049890"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'katagori': ['Abusive', 'HS_Individual', 'HS_Race', 'HS_Gender', 'HS_Other']}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[],"metadata":{"id":"fiU2jKXvdk13"},"execution_count":null,"outputs":[]}]}